{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528713c7-2793-4af4-880d-eda530cb83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ee746b-eab0-4a51-89b2-62f863c1e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9accf899-feb2-4aae-98c0-1e200100b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6baba3-8d07-49ea-9a31-97238957e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911eeab-3ebe-45a8-b0d9-33abaf8cc4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7abf6b2-9da8-4105-93f5-0c2529f7ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a855ed7-053b-4a78-9321-2ae17290cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990a875a-d80a-46f0-a1d1-05c26d8ac5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = model_output[0]\n",
    "token_embeddings.shape\n",
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "mean_pooling  = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37990e4-ed9e-46dd-b2a0-747cbfb9daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pooling\n",
    "#sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(mean_pooling, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0209260a-a414-411a-8203-3f7eb8093c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e4eb32b-8ce5-479c-9be1-090e18ebf5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60ec148e-8874-4e92-9ad5-8fe74a555239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example sentence', 'Each sentence is converted']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c39e8a81-d32e-42e9-b609-0e41886732ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentence_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(modules\u001b[38;5;241m=\u001b[39m[word_embedding_model])\n\u001b[0;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bunka/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:180\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    178\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Sentence embeddings\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mout_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_value\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    181\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_embeddings:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentence_embedding'"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(modules=[word_embedding_model])\n",
    "res = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e25fa44-b034-4068-9594-43de491b296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2436d018-fbb0-4a71-9272-791662232eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['that that This is an example sentence that I like a lot and I will tell you that I', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6568cd8-1a42-410c-bd1b-5c3f85c2ceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b36aa58-1a72-4218-b9c4-fe8e5e2b84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d859163e-1f30-4b05-8da5-76e34a3fdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d54db1-78f8-4cc1-95f4-b923a969e34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ecf9badd-4c95-4a4c-a0b4-4015a7d1dff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2008, 2008, 2023, 2003, 2019, 2742, 6251, 2008, 1045, 2066, 1037,\n",
       "        2843, 1998, 1045, 2097, 2425, 2017, 2008, 1045,  102])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "baa30299-5256-447d-a471-7e9a3af1f58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2169, 6251, 2003, 4991,  102,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2ab9dca-029a-4d0d-b177-05f0fd0fb9c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mEmbedding\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-similarity-davinci-001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "response = openai.Embedding.create(\n",
    "    input=\"This is an example\",\n",
    "    engine=\"text-similarity-davinci-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2425750a-b3b6-4275-9329-e39a39632c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting openai\n",
      "  Downloading openai-0.16.0.tar.gz (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.3 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from openai) (1.4.1)\n",
      "Collecting openpyxl>=3.0.7\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from openai) (4.63.0)\n",
      "Collecting pandas-stubs>=1.1.0.11\n",
      "  Downloading pandas_stubs-1.2.0.53-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 96.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from openai) (2.27.1)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from pandas>=1.2.3->openai) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from pandas>=1.2.3->openai) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.3->openai) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/charlesdedampierre/opt/anaconda3/envs/bunka/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.8)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai: filename=openai-0.16.0-py3-none-any.whl size=50782 sha256=447f6e1326041d2b84e0ba755d46ec0a2b0b5200b3d834729a1ef41e2e2e93fd\n",
      "  Stored in directory: /Users/charlesdedampierre/Library/Caches/pip/wheels/66/4c/15/b6dd0dd2f66564df02055f72920b7c69b94b0cddb8cc160ae6\n",
      "Successfully built openai\n",
      "Installing collected packages: et-xmlfile, pandas-stubs, openpyxl, openai\n",
      "Successfully installed et-xmlfile-1.1.0 openai-0.16.0 openpyxl-3.0.9 pandas-stubs-1.2.0.53\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83579f9f-3f31-464a-a092-d11a9b077b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
